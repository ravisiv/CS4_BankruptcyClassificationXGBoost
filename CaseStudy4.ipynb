{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "789e574a",
   "metadata": {},
   "source": [
    "### Case Study 4 : Financial Delinquency\n",
    "\n",
    "Submitted by:\n",
    "\n",
    "- Ravi Sivaraman\n",
    "- Balaji Avvaru\n",
    "- Apurv Mittal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b98e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve, accuracy_score, confusion_matrix, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize, StandardScaler\n",
    "from sklearn import metrics as mt\n",
    "import getpass\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ba29b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/ravis/Documents/GitHub/QTW-CaseStudy4/Data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rr/gnmws3gs2xn3p2l0t4_lq7hc0000gn/T/ipykernel_64799/3407772379.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# get all data files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdata_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/Data'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/Data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/ravis/Documents/GitHub/QTW-CaseStudy4/Data'"
     ]
    }
   ],
   "source": [
    "current_user = getpass.getuser()\n",
    "\n",
    "if current_user == 'balaj':\n",
    "    data_path = \"/Users/balaj/OneDrive/Desktop/Docs/Docs 1/SMU/MSDS 7333/Case Study 4/Data\"\n",
    "elif current_user == 'ravis':\n",
    "    data_path = \"/Users/ravis/Library/CloudStorage/OneDrive-SouthernMethodistUniversity/Case Study 4/Data\"\n",
    "elif current_user == \"apurv\":\n",
    "    data_path = \"/Users/apurv/Library/CloudStorage/OneDrive-SouthernMethodistUniversity/SMU/7333 - QTW/Case Study 4/Data\"\n",
    "\n",
    "# get all data files\n",
    "data_files = [f for f in os.listdir(data_path) if (not f.startswith('.')) and os.path.isfile(join(data_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb9d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66b25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for f in data_files:\n",
    "    data_temp = arff.loadarff(os.getcwd()+'/Data/'+f)\n",
    "    temp_df = pd.DataFrame(data_temp[0])\n",
    "    df = df.append(temp_df, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb14852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604eaed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d2eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d8924",
   "metadata": {},
   "source": [
    "##### Missing value analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate null values in the csv file\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4983d700",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "# Validate null values in the csv file\n",
    "print((df.isnull().sum()).sort_values(ascending=False))\n",
    "# print columns with null values\n",
    "missing_data_columns = df.columns[df.isnull().any()]\n",
    "print(\"\\n\\nColumns with null values\")\n",
    "print(\"************************\")\n",
    "missing_data_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f02347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of missing values in each variable\n",
    "(df[missing_data_columns].isnull().sum()/len(df)*100).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770f2c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top5_missing_data_col = ['Attr37', 'Attr21', 'Attr27', 'Attr60', 'Attr45']\n",
    "df[top5_missing_data_col].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7037e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill NA with median() of each column in dataset\n",
    "df = df.apply(lambda x: x.fillna(x.median()),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4014c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate null values in the csv file\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c246e97c",
   "metadata": {},
   "source": [
    "##### Independent Variable analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951285cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the hist of data to check normality of independent variable\n",
    "df_X = df.drop(['class'],axis=1)\n",
    "df_X.hist(bins=50,figsize=(25,30))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df886837",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#heatmap - correlation matrix\n",
    "plt.figure(figsize=(55, 50)) #code reference (5-1)\n",
    "sns.heatmap(df_X.corr(), annot=True)\n",
    "plt.title('HeatMap-Correlation Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dabcdd8",
   "metadata": {},
   "source": [
    "##### Check for Multicolliniarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9467f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://www.projectpro.io/recipes/drop-out-highly-correlated-features-in-python\n",
    "# to drop features with colliniarity more than 95%\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "corr_df = pd.DataFrame(df_X.corr().abs())\n",
    "corr_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c3f9e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Multi Colliniarity analysis on Independent variables \n",
    "upper_tri = corr_df.where(np.triu(np.ones(corr_df.shape),k=1).astype(np.bool))\n",
    "print(upper_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92058221",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9999)]\n",
    "print((to_drop))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891ff18f",
   "metadata": {},
   "source": [
    "#### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71924e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df['class'].replace([b'0', b'1'], [0, 1])\n",
    "\n",
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a1885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = \"class\", data = df)\n",
    "plt.title(\"Distribution of Target Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a1d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart\n",
    "df['class'].value_counts().plot.pie(autopct = \"%.1f%%\")\n",
    "plt.title(\"Proportion of Target Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b025010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['class'],axis=1)\n",
    "ind_columns = df.drop('class',axis=1).columns\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65efcf04",
   "metadata": {},
   "source": [
    "We did normalize the attributes using StandardScaler() to scale them between 0 and 1 before running models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df0e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410c38c",
   "metadata": {},
   "source": [
    "We chose a stratified k-fold validation algorithm. In stratified k-fold cross-validation, the original sample is randomly partitioned into k equal size subsamples in which each fold contains roughly the same proportions of class labels. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k-1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data. The k results from the folds can then be averaged (or otherwise combined) to produce a single estimation. The advantage of this method is that all observations are used for both training and validation, and each observation is used for validation exactly once.\n",
    "\n",
    "The typical standard of 10 folds will be adequate for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Cross Validation Procedure\n",
    "cv = StratifiedKFold(n_splits=10, random_state=1234, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112da47b",
   "metadata": {},
   "source": [
    "#### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93363874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Metrics\n",
    "def displayModel_metrics(best_model, grid_model, features, target, cv):   \n",
    "    start = time.time()\n",
    "    cv_results = cross_validate(best_model, features, target, cv=cv, scoring=['accuracy','precision','recall','f1'], n_jobs=-1)\n",
    "    elapsed_time = (time.time() - start) \n",
    "    print ('Fold Scores:')\n",
    "    print(' ')\n",
    "    print(cv_results['test_accuracy'])\n",
    "    print(' ')\n",
    "    print('Best Accuracy   :  {:.3f}'.format(grid_model.best_score_))\n",
    "    print('Mean Accuracy   : ', cv_results['test_accuracy'].mean())\n",
    "    print('Mean Precision  : ', cv_results['test_precision'].mean())\n",
    "    print('Mean Recall     : ', cv_results['test_recall'].mean())\n",
    "    print('Mean F-Score   : ', cv_results['fscore'].mean())\n",
    "    print('Mean Fit Time   : ', cv_results['fit_time'].mean())\n",
    "    print('Mean Score Time : ', cv_results['score_time'].mean())\n",
    "    print('CV Time         : ', elapsed_time)\n",
    "    return\n",
    "\n",
    "# ROC curve plot\n",
    "def roc_curve_plot(model_fit, features, target):\n",
    "\n",
    "    sns.set_palette(\"dark\")\n",
    "\n",
    "    yhat_score = model_fit.predict_proba(features)\n",
    "\n",
    "    # Compute ROC curve for a subset of interesting classes\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in np.unique(target):\n",
    "        fpr[i], tpr[i], _ = mt.roc_curve(y, yhat_score[:, i], pos_label=i)\n",
    "        roc_auc[i] = mt.auc(fpr[i], tpr[i])\n",
    "\n",
    "    for i in np.unique(target):\n",
    "        plt.plot(fpr[i], tpr[i], label= ('class %d (area = %0.2f)' % (i, roc_auc[i])))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "    plt.legend(loc=\"lower right\")  \n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef96a15c",
   "metadata": {},
   "source": [
    "#### Model 1: Random Forest with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(class_weight='balanced', random_state=1234)\n",
    "rf_clf.fit(X_scaled, y)\n",
    "\n",
    "rf_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c428ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = rf_clf.predict(X_scaled)\n",
    "accuracy_score(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b7b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b379d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve for Random Forest Classifier\n",
    "roc_curve_plot(rf_clf, X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40029ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_precision_recall_curve(rf_clf, X_scaled, y)\n",
    "disp.ax_.set_title('Precision-Recall Curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a567714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "cv_df = pd.DataFrame(cross_validate(rf_clf, X_scaled, y, cv=cv, scoring=['accuracy','precision','recall', 'f1'], n_jobs=-1))\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5165e9",
   "metadata": {},
   "source": [
    "#### Model 2: Random Forest with GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa34ede",
   "metadata": {},
   "source": [
    "Random forest is an ensemble tree-based learning algorithm where it combines more than one algorithms of same or different kind for classifying objects. The Random Forest Classifier is a set of decision trees from randomly selected subset of training set. It aggregates the votes from different decision trees to decide the final class of the test object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f15d7c",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "\n",
    "- n_estimators: number of trees in the forest\n",
    "\n",
    "- max_depth: max number of levels in each decision tree\n",
    "\n",
    "- criterion: The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific\n",
    "\n",
    "- min_samples_split = min number of data points placed in a node before the node is split\n",
    "\n",
    "- min_samples_leaf = min number of data points allowed in a leaf node\n",
    "\n",
    "- class_weight: The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier()\n",
    "\n",
    "# define parameters       \n",
    "max_depth_RF = [5, 7, 8, 10, 12]\n",
    "random_state_RF = [1234]\n",
    "n_estimators_RF =  [100]\n",
    "criterion_RF = ['entropy']\n",
    "min_samples_leaf_RF = [3, 4, 5]\n",
    "min_samples_split_RF = [8, 10, 12]\n",
    "class_weight_RF = ['balanced']\n",
    "\n",
    "# define grid search\n",
    "# param_grid_RF = dict(n_estimators=n_estimators_RF, max_depth=max_depth_RF, random_state=random_state_RF,\n",
    "#                      criterion=criterion_RF, min_samples_leaf=min_samples_leaf_RF,\n",
    "#                     min_samples_split=min_samples_split_RF, class_weight=class_weight_RF)\n",
    "\n",
    "# search_RF = GridSearchCV(estimator=RF, param_grid=param_grid_RF, n_jobs=3, cv=cv, \n",
    "#                                scoring='accuracy',error_score=0, verbose=1)\n",
    "\n",
    "\n",
    "# define random search\n",
    "param_random_RF = dict(n_estimators=n_estimators_RF, max_depth=max_depth_RF, random_state=random_state_RF,\n",
    "                     criterion=criterion_RF, min_samples_leaf=min_samples_leaf_RF,\n",
    "                    min_samples_split=min_samples_split_RF, class_weight=class_weight_RF)\n",
    "\n",
    "\n",
    "search_RF = RandomizedSearchCV(estimator=RF, param_distributions=param_random_RF, n_jobs=3, cv=cv, \n",
    "                               scoring='accuracy',n_iter=20, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee405b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result_RF = search_RF.fit(X_scaled, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (result_RF.best_score_, result_RF.best_params_))\n",
    "means = result_RF.cv_results_['mean_test_score']\n",
    "stds = result_RF.cv_results_['std_test_score']\n",
    "params = result_RF.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9add293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The GridSearch algorithm determined the following optimal parameters\n",
    "best_Estimator_RF =result_RF.best_estimator_\n",
    "Coef_weights_RF = result_RF.best_estimator_.feature_importances_\n",
    "best_Estimator_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d12b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model metrics\n",
    "displayModel_metrics(best_Estimator_RF, result_RF, X_scaled, y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d97eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = cross_val_predict(best_Estimator_RF, X_scaled, y, cv=10)\n",
    "conf_mat = confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab6741",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb8955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve for Random Forest Classifier\n",
    "roc_curve_plot(result_RF, X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b877641",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_precision_recall_curve(best_Estimator_RF, X_scaled, y)\n",
    "disp.ax_.set_title('Precision-Recall Curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c128dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "precision, recall, threshold = metrics.precision_recall_curve(y, best_Estimator_RF.predict_proba(X_scaled)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78efe0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "plt.plot(threshold, precision[:-1], label='Precision')\n",
    "plt.plot(threshold, recall[:-1], label='Recall')\n",
    "plt.xlabel('Thresold')\n",
    "plt.ylabel('Proportion')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecf707e",
   "metadata": {},
   "source": [
    "If we wanted to ensure that the model classify 95% of the financial institutions that go bankrupt  correctly we would want to select the following threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mal95_ind = np.argmin(recall >= 0.95)-1\n",
    "mal95_thresh = threshold[mal95_ind]\n",
    "mal95_precision = precision[mal95_ind]\n",
    "mal95_recall = recall[mal95_ind]\n",
    "\n",
    "print(\"Threshold:\", mal95_thresh)\n",
    "print(\"Precision:\", mal95_precision)\n",
    "print(\"Recall:\", mal95_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4605f320",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4963ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important features with their weights \n",
    "imp_feature_df = pd.DataFrame({'feature_names':ind_columns, \n",
    "                               'Coef_weights':Coef_weights_RF})\n",
    "imp_feature_df.sort_values(by='Coef_weights', inplace=True, ascending=False )\n",
    "\n",
    "imp_feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visulization of important features \n",
    "%matplotlib inline\n",
    "\n",
    "ax = sns.barplot(x ='Coef_weights', y = 'feature_names',data=imp_feature_df.head(10), orient= 'h')\n",
    "ax.set_title(\"Random Forest Feature Importance\")\n",
    "ax.set_xlabel(\"Coefficient Magnitude\\n(z-score)\")\n",
    "ax.set_ylabel(\"Feature Names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369ab67",
   "metadata": {},
   "source": [
    "#### Model 3: XGBoost with default parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBClassifier(random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf.fit(X_scaled, y)\n",
    "\n",
    "xgb_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d15d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = xgb_clf.predict(X_scaled)\n",
    "accuracy_score(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeccccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b936280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve for XGB Classifier\n",
    "roc_curve_plot(xgb_clf, X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a5a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_precision_recall_curve(xgb_clf, X_scaled, y)\n",
    "disp.ax_.set_title('Precision-Recall Curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1516c9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "cv_df = pd.DataFrame(cross_validate(xgb_clf, X_scaled, y, cv=cv, scoring=['accuracy','precision','recall', 'f1'], n_jobs=-1))\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ea5f4",
   "metadata": {},
   "source": [
    "#### Model 4: XGBoost with GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f1695b",
   "metadata": {},
   "source": [
    "Parameters\n",
    "\n",
    "- learning_rate: The learning rate. In each boosting step, this values shrinks the weight of new features, preventing overfitting or a local minimum. This value must be between 0 and 1. The default value is 0.3.\n",
    "\n",
    "- max_depth: The maximum depth of a tree. Be careful, greater the depth, greater the complexity of the model and more easy to overfit. This value must be an integer greater than 0 and have 6 as default.\n",
    "\n",
    "- n_estimators: The number of trees in our ensemble.\n",
    "\n",
    "- gamma: A regularization term and it’s related to the complexity of the model. It’s the minimum loss necessary to occur a - -split in a leaf. It can be any value greater than zero and has a default value of 0.\n",
    "\n",
    "- colsample_bytree: Represents the fraction of columns to be subsampled. It’s related to the speed of the algorithm and prevent overfitting. Default value is 1 but it can be any number between 0 and 1.\n",
    "\n",
    "- lambda: L2 regularization on the weights. This encourages smaller weights. Default is 1 but it can be any value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d426e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB = xgb.XGBClassifier()\n",
    "\n",
    "# define parameters       \n",
    "clf_n_estimators_XGB = [200]\n",
    "# clf_learning_rate_XGB =  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5]   0.3\n",
    "# clf_max_depth_XGB = range(3, 15) 5\n",
    "# clf_colsample_bytree_XFB = [i/10.0 for i in range(1, 3)]  0.2\n",
    "# clf_gamma_XGB = [i/10.0 for i in range(1, 8)]          0.1\n",
    "# lambda_XGB = [0.1, 1.0, 5.0, 10.0, 50.0, 100.0]        0.1\n",
    "# min_child_weight = [0.1, 0.9, 0.95]                    1\n",
    "\n",
    "clf_learning_rate_XGB =  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5]\t \n",
    "clf_max_depth_XGB = range(3, 15)\t\t\t\t\t\t \n",
    "clf_colsample_bytree_XFB = [i/10.0 for i in range(1, 3)]\t\t \n",
    "clf_gamma_XGB = [0.01, 0.05, 0.1, 0.2, 0.3]\t\t\t\t \n",
    "lambda_XGB = [0.01, 0.05, 0.1, 1.0, 5.0, 10.0, 50.0, 100.0]\t\t \n",
    "min_child_weight = [0.1, 0.9, 0.95, 2, 3]\t\n",
    "random_state_XGB = [1234]\n",
    "\n",
    "# define grid search\n",
    "# param_grid_RF = dict(n_estimators=clf_n_estimators_XGB, learning_rate=clf_learning_rate_XGB, \n",
    "#                      max_depth=clf_max_depth_XGB, colsample_bytree = clf_colsample_bytree_XFB,\n",
    "#                     gamma=clf_gamma_XGB, reg_lambda=lambda_XGB)\n",
    "\n",
    "# search_RF = GridSearchCV(estimator=RF, param_grid=param_grid_RF, n_jobs=3, cv=cv, \n",
    "#                                scoring='accuracy',error_score=0, verbose=1)\n",
    "\n",
    "\n",
    "# define random search\n",
    "param_random_XGB = dict(n_estimators=clf_n_estimators_XGB, learning_rate=clf_learning_rate_XGB, \n",
    "                     max_depth=clf_max_depth_XGB, colsample_bytree = clf_colsample_bytree_XFB,\n",
    "                    gamma=clf_gamma_XGB, reg_lambda=lambda_XGB, random_state=random_state_XGB)\n",
    "\n",
    "\n",
    "search_XGB = RandomizedSearchCV(estimator=XGB, param_distributions=param_random_XGB, n_jobs=3, cv=cv, \n",
    "                               scoring='accuracy',n_iter=20, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54f8cb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result_XGB = search_XGB.fit(X_scaled, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (result_XGB.best_score_, result_XGB.best_params_))\n",
    "means = result_XGB.cv_results_['mean_test_score']\n",
    "stds = result_XGB.cv_results_['std_test_score']\n",
    "params = result_XGB.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142803f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The GridSearch algorithm determined the following optimal parameters\n",
    "best_Estimator_XGB =result_XGB.best_estimator_\n",
    "Coef_weights_XGB = result_XGB.best_estimator_.feature_importances_\n",
    "best_Estimator_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc4507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model metrics\n",
    "displayModel_metrics(best_Estimator_XGB, result_XGB, X_scaled, y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f680c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve for Random Forest Classifier\n",
    "roc_curve_plot(result_XGB, X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63087bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_precision_recall_curve(best_Estimator_XGB, X_scaled, y)\n",
    "disp.ax_.set_title('Precision-Recall Curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, threshold = metrics.precision_recall_curve(y, best_Estimator_XGB.predict_proba(X_scaled)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9950e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(threshold, precision[:-1], label='Precision')\n",
    "plt.plot(threshold, recall[:-1], label='Recall')\n",
    "plt.xlabel('Thresold')\n",
    "plt.ylabel('Proportion')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076e7d1d",
   "metadata": {},
   "source": [
    "If we wanted to ensure that the model classify 95% of the financial institutions that go bankrupt  correctly we would want to select the following threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c27e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mal95_ind = np.argmin(recall >= 0.95)-1\n",
    "mal95_thresh = threshold[mal95_ind]\n",
    "mal95_precision = precision[mal95_ind]\n",
    "mal95_recall = recall[mal95_ind]\n",
    "\n",
    "print(\"Threshold:\", mal95_thresh)\n",
    "print(\"Precision:\", mal95_precision)\n",
    "print(\"Recall:\", mal95_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2d12a2",
   "metadata": {},
   "source": [
    "### Feature Importance with XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42e6e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important features with their weights \n",
    "imp_feature_df = pd.DataFrame({'feature_names':ind_columns, \n",
    "                               'Coef_weights':Coef_weights_XGB})\n",
    "imp_feature_df.sort_values(by='Coef_weights', inplace=True, ascending=False )\n",
    "\n",
    "imp_feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24685442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visulization of important features \n",
    "%matplotlib inline\n",
    "\n",
    "ax = sns.barplot(x ='Coef_weights', y = 'feature_names',data=imp_feature_df.head(10), orient= 'h')\n",
    "ax.set_title(\"XGB Feature Importance\")\n",
    "ax.set_xlabel(\"Coefficient Magnitude\\n(z-score)\")\n",
    "ax.set_ylabel(\"Feature Names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b429325d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
